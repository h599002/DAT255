{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to activation functions and parameter initialisers\n",
    "\n",
    "In this notebook we will have a quick look at activation functions and how the choice of parameter initialisation methods affect the training of a neural network. \n",
    "\n",
    "As a little reminder, the _activation function_ is a nonlinear transformation applied to the output values of each node in the network. Before we start training the network, the parameters (weights) of each connection between the nodes must be set to some value. We choose these values at random, but randoms numbers always come from some _distribution_, and there are good ways and less-good ways of selecting this distribution. \n",
    "\n",
    "Tuning the parameter values are done by computing the _gradients_ with respect to the loss, and here we will write our own training loop (instead of using `model.fit()`), so that we can study these gradients in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Let us plot some activation functions, in order to compare them. Let's start with ReLU and the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 100)\n",
    "\n",
    "relu = keras.activations.relu(x)\n",
    "sigmoid = keras.activations.sigmoid(x)\n",
    "\n",
    "plt.plot(x, relu, label='ReLU')\n",
    "plt.plot(x, sigmoid, label='Sigmoid')\n",
    "plt.xlim((-4, 4))\n",
    "plt.ylim((-1.5, 2.0))\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: red;\">Exercise:<span>\n",
    "\n",
    "Add the other activation functions available in Keras (listed [here](https://keras.io/api/layers/activations/)), and plot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "In this example we stick to very simple data -- classification of 2-dimensional data, which form two circles.\n",
    "\n",
    "Scikit-learn has a couple of convenience functions for creating simple datasets, and you can also choose some other data if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X_train, y_train = make_circles(n_samples=500, noise=0.1)\n",
    "X_test, y_test = make_circles(n_samples=100, noise=0.1)\n",
    "\n",
    "plt.scatter(x=X_train[:, 0], y=X_train[:, 1], c=y_train, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now in NumPy format, which is in principle fine, but for our custom training loop it will be more practical to have the data in TensorFlow Dataset (`tf.data.Dataset`) from. We get back to this next week, but for now, let's just accept the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Here comes the exciting bit, our custom training loop, which computes the gradients and tunes the network parameters accordingly.\n",
    "\n",
    "To compute the gradients, we need to run our model and compute the loss, and then the `GradientTape` fixes the rest. For this experiment we want to keep track of the gradients so we can plot them later, but remember there is one gradient per parameter in the network, which is usually a lot. To reduce the information a bit we can either compute the average of all of them, or we can also compute the L2 norm, which we know from before. In this case we use the L2 norm, but you are free to change it. In any case, let's average over the gradients in each layer individually, so we can see what the different layers are up to.\n",
    "\n",
    "More information about custom training loops can be found here: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimiser, loss_function, num_epochs=100):\n",
    "\n",
    "    # Log the gradients\n",
    "    gradients_L2 = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Start of epoch {epoch}')\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Run the forward pass of the layer.\n",
    "                # The operations that the layer applies\n",
    "                # to its inputs are going to be recorded\n",
    "                # on the GradientTape.\n",
    "                preds = model(x_batch_train, training=True)\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                loss_value = loss_function(y_batch_train, preds)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "            # Extract the L2 norm of the gradients \n",
    "            L2 = []\n",
    "            for grad in grads:\n",
    "                if len(grad.shape) != 1:    # skip biases, look only at weights\n",
    "                    L2.append(tf.norm(grad).numpy())\n",
    "\n",
    "            gradients_L2.append(L2)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimiser.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Epoch end\n",
    "        print(f'Epoch {epoch}: train loss = {model.evaluate(train_dataset, verbose=0)}, test loss = {model.evaluate(test_dataset,verbose=0)}')\n",
    "\n",
    "        return gradients_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Now, since we want to test out different activation function and initialiser combinations, we can define a function that creates a neural network for the given combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation, initializer):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(2,)))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=10, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(keras.layers.Dense(units=10, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(keras.layers.Dense(units=10, activation=activation, kernel_initializer=initializer))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first test, let's try a configuration that was common in the 90s: The sigmoid activation function, and initialising the parameters from a uniform (i.e. a flat) distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "activation = 'sigmoid'\n",
    "#initializer = keras.initializers.HeUniform()\n",
    "#activation = 'relu'\n",
    "\n",
    "# Create model\n",
    "sigmoid_model = create_model(activation, initializer)\n",
    "\n",
    "# Compile it\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.5)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "sigmoid_model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the first model\n",
    "\n",
    "Time to train the first model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_L2 = training_loop(sigmoid_model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the first model\n",
    "\n",
    "First, investigate the model's performance simply by looking at how the _decision boundary_ matches with the test data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def plot_decision_boundary(model):\n",
    "\n",
    "    feature_1, feature_2 = np.meshgrid(\n",
    "        np.linspace(-1.5, 1.5, 20),\n",
    "        np.linspace(-1.5, 1.5, 20)\n",
    "    )\n",
    "\n",
    "    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "    y_pred = np.reshape(model.predict(grid), feature_1.shape)\n",
    "\n",
    "    display = DecisionBoundaryDisplay(\n",
    "        xx0=feature_1, xx1=feature_2, response=y_pred\n",
    "    )\n",
    "    display.plot()\n",
    "\n",
    "    plt.scatter(x=X_test[:, 0], y=X_test[:, 1], c=y_test, cmap='viridis')\n",
    "\n",
    "\n",
    "\n",
    "plot_decision_boundary(sigmoid_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we look at the history of the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_L2 = np.array(grads_L2)\n",
    "\n",
    "for i in range(grads_L2.shape[1]):\n",
    "    plt.plot(np.arange(grads_L2.shape[0]), grads_L2[:,i], label=f'Layer {i}', alpha=0.7)\n",
    "\n",
    "plt.ylim([0,0.08])\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the second model\n",
    "\n",
    "This time around, we try a similar model but where activation function and initialistion methods is chosen like the textbook recommends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = keras.initializers.HeUniform()\n",
    "activation = 'relu'\n",
    "\n",
    "# Create model\n",
    "relu_model = create_model(activation, initializer)\n",
    "\n",
    "# Compile it\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.5)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "relu_model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_L2 = training_loop(relu_model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(relu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_L2 = np.array(grads_L2)\n",
    "\n",
    "for i in range(grads_L2.shape[1]):\n",
    "    plt.plot(np.arange(grads_L2.shape[0]), grads_L2[:,i], label=f'Layer {i}', alpha=0.7)\n",
    "\n",
    "plt.ylim([0,0.08])\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: red;\">Exercise:<span>\n",
    "\n",
    "1) Look at the decision boundaries. Which model performs best? Can you make them perform equally good by changing the learning rate?\n",
    "2) Look at the gradient plots. Do the different layers in the model show equal gradient values? What can you say by comparing the curves?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
